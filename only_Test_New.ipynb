{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KecvMv1SRtm"
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rwGVRHTISRtr",
    "outputId": "396d2059-7f0a-422b-86aa-ed66ac75d450"
   },
   "outputs": [],
   "source": [
    "# COCO related libraries\n",
    "from samples.coco import coco\n",
    "\n",
    "# MaskRCNN libraries\n",
    "from mrcnn.config import Config\n",
    "\n",
    "\n",
    "\n",
    "import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "import mrcnn.model_resnext_2_0_v3_nms as modellib\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32njXW7eSRtt"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyAUsqxwSRtu"
   },
   "source": [
    "## Additional setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l-e-QYoJSRtu"
   },
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Select which GPU to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zskUS2nVSRtu"
   },
   "source": [
    "## Include evaluation scripts in training script so that the kernel does not have to be reloaded. Eases the process of rapidly training and evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7ZwfGabSRtu"
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uesHiSsASRtv"
   },
   "outputs": [],
   "source": [
    "# COCO related libraries\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "from samples.coco import coco\n",
    "from samples.coco.coco import evaluate_coco\n",
    "\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import skimage.io\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7h5Yu_geSRtw"
   },
   "outputs": [],
   "source": [
    "# Number of classes in dataset. Must be of type integer\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# Relative path to .h5 weights file\n",
    "#WEIGHTS_FILE = None\n",
    "\n",
    "WEIGHTS_FILE = 'logs/mask_rcnn_5_classes_resnetxt_161_4x4_epc_250_step_per_ep_220_val_steps_100_alldataset_aug_v1_alpha_1_0219.h5'\n",
    "\n",
    "# Relative path to ground truth annotations JSON file\n",
    "#TEST_ANNOTATIONS_FILE ='datasets/aug_1/test/test.json'\n",
    "TEST_ANNOTATIONS_FILE ='datasets/aug_1/test/test.json'\n",
    "\n",
    "\n",
    "# Relative path to images associated with ground truth JSON file\n",
    "TEST_DATASET_DIR = 'datasets/aug_1/test/'\n",
    "\n",
    "# Relative path to the directory of images that you want to run inferencing on\n",
    "TEST_IMAGE_DIR = 'datasets/aug_1/test/'\n",
    "\n",
    "MODEL_NAME = \"test\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8Vc-cFHSRtw"
   },
   "source": [
    "## Declare evaluation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yu4PJ6MQSRtw"
   },
   "outputs": [],
   "source": [
    "class EvalConfig(coco.CocoConfig):\n",
    "    \"\"\" Configuration for evaluation \"\"\"\n",
    "    \n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = MODEL_NAME\n",
    "    \n",
    "    # How many GPUs\n",
    "    GPU_COUNT = 1\n",
    "    \n",
    "    # How many images per gpu\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + NUM_CLASSES  # background + other classes\n",
    "    \n",
    "    #layers\n",
    "    FPN_CLASSIF_FC_LAYERS_SIZE = 1024\n",
    "    \n",
    "    IMAGE_MIN_DIM = 1152\n",
    "    IMAGE_MAX_DIM = 1280\n",
    "    \n",
    "    # Alpha for changing mask loss weights\n",
    "    ALPHA =1.\n",
    "    \n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": ALPHA* 1.\n",
    "    }    \n",
    "    \n",
    "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
    "    # [\"resnet50\", \"resnet101\", \"resnet152\", \"resnet203\", \"resnetxt50\", \"resnetxt101\", \"resnetxt152\",  \"resnetxt203\"]\n",
    "    BACKBONE = 'resnetxt152'\n",
    "    \n",
    "    CARDINALITY = 32\n",
    "\n",
    "\n",
    "\n",
    "    # To be honest, I haven't taken the time to figure out what these do\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    \n",
    "    # Changed to 512 because that's how many the original MaskRCNN paper used\n",
    "    TRAIN_ROIS_PER_IMAGE = 512#200\n",
    "    MAX_GT_INSTANCES = 256#114\n",
    "    POST_NMS_ROIS_INFERENCE = 2000#1000 \n",
    "    POST_NMS_ROIS_TRAINING = 2000 \n",
    "    \n",
    "    DETECTION_MAX_INSTANCES = 400#114\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcuQfhN_SRtx"
   },
   "source": [
    "## Display configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YfMbYT_HSRtx",
    "outputId": "0fe83b5b-daad-4556-ccab-8c2ab3fff339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "ALPHA                          1.0\n",
      "BACKBONE                       resnetxt152\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "CARDINALITY                    32\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        400\n",
      "DETECTION_MIN_CONFIDENCE       0.5\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1280\n",
      "IMAGE_META_SIZE                18\n",
      "IMAGE_MIN_DIM                  1152\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1280 1280    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               256\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           test\n",
      "NUM_CLASSES                    6\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        2000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           512\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EvalConfig().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJMfm1EGSRtx"
   },
   "source": [
    "## Build class to load ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bEqTRh5OSRtx"
   },
   "outputs": [],
   "source": [
    "class CocoDataset(utils.Dataset):\n",
    "    def load_coco_gt(self, annotations_file, dataset_dir):\n",
    "        \"\"\"Load a COCO styled ground truth dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create COCO object\n",
    "        coco = COCO(annotations_file)\n",
    "\n",
    "        # Load all classes\n",
    "        class_ids = sorted(coco.getCatIds())\n",
    "\n",
    "        # Load all images\n",
    "        image_ids = list(coco.imgs.keys())\n",
    "\n",
    "        # Add classes\n",
    "        for i in class_ids:\n",
    "            self.add_class(\"coco\", i, coco.loadCats(i)[0][\"name\"])\n",
    "\n",
    "        # Add images\n",
    "        for i in image_ids:\n",
    "            self.add_image(\n",
    "                \"coco\", image_id = i,\n",
    "                path = os.path.join(dataset_dir, coco.imgs[i]['file_name']),\n",
    "                width = coco.imgs[i][\"width\"],\n",
    "                height = coco.imgs[i][\"height\"],\n",
    "                annotations = coco.loadAnns(coco.getAnnIds(\n",
    "                    imgIds = [i], catIds = class_ids, iscrowd=None)))\n",
    "        \n",
    "        return coco\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"coco\":\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"coco.{}\".format(annotation['category_id']))\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"coco\":\n",
    "            return \"http://cocodataset.org/#explore?id={}\".format(info[\"id\"])\n",
    "        else:\n",
    "            super(CocoDataset, self).image_reference(image_id)\n",
    "\n",
    "    # The following two functions are from pycocotools with a few changes.\n",
    "\n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDP2ydrsSRty"
   },
   "source": [
    "## Build MaskRCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "07DnLlm6SRty",
    "outputId": "469f366f-17b2-4e39-ed57-6badba82e492",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinality, network, blocks:  32 resnetxt152 35\n",
      "Metal device set to: Apple M1\n",
      "prop Tensor(\"ROI/strided_slice_21:0\", shape=(None, 4), dtype=float32)\n",
      "soft nms Tensor(\"ROI/packed_2:0\", shape=(1, None, 4), dtype=float32)\n",
      "keep antes Tensor(\"mrcnn_detection/strided_slice_22:0\", shape=(None,), dtype=int64)\n",
      "keep Tensor(\"mrcnn_detection/strided_slice_24:0\", shape=(None,), dtype=int64)\n",
      "WARNING:tensorflow:From /Users/carolinarutilidelima/opt/miniconda3/envs/mlp/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "N None\n",
      "final class keep Tensor(\"mrcnn_detection/map/while/PadV2:0\", shape=(400,), dtype=int64)\n",
      "Tensor(\"mrcnn_detection/Pad:0\", shape=(None, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the model in inference mode\n",
    "model = modellib.MaskRCNN(mode = \"inference\", config = EvalConfig(), model_dir = MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrTXkSQXSRtz"
   },
   "source": [
    "## Load weights from last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfj3W1peSRtz",
    "outputId": "58ee44f5-39fc-489a-9f79-4d64fd6690b0"
   },
   "outputs": [],
   "source": [
    "if WEIGHTS_FILE is None:\n",
    "    model.load_weights(model.find_last(), by_name = True)\n",
    "else:\n",
    "    model.load_weights(WEIGHTS_FILE, by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKWnerTrSRtz"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b70h_b69SRtz",
    "outputId": "9f5395bd-d38a-4d49-cc72-111c903f73b2"
   },
   "outputs": [],
   "source": [
    "dataset_val = CocoDataset()\n",
    "coco = dataset_val.load_coco_gt(annotations_file = TEST_ANNOTATIONS_FILE, dataset_dir = TEST_DATASET_DIR)\n",
    "dataset_val.prepare()\n",
    "class_names = dataset_val.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4qkZ65USRtz",
    "outputId": "45982823-cac3-445c-b8d6-e12d802dca62"
   },
   "outputs": [],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5rR1K2lSRtz"
   },
   "source": [
    "## Evaluate model with COCO test\n",
    "### If your results come back as a bunch of zeros, check to make sure that the \"width\" and \"height\" tag in your COCO dataset are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeiZwVNzSRt0"
   },
   "outputs": [],
   "source": [
    "#evaluate_coco(model, dataset_val, coco, \"segm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvcjBgGhSRt0"
   },
   "source": [
    "## Calculating mAP as per example in train_shapes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "38af6c05fe7747c9a7fae8987714478c"
     ]
    },
    "id": "feGasGxlSRt0",
    "outputId": "80b4c979-4017-4257-f030-74f8fee9731f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, len(dataset_val.image_ids))\n",
    "\n",
    "# Instanciate arrays to create our metrics\n",
    "APs = []\n",
    "ARs = []\n",
    "precisions_arr = []\n",
    "recalls_arr = []\n",
    "overlaps_arr = []\n",
    "class_ids_arr = []\n",
    "scores_arr = []\n",
    "F1_scores = []; \n",
    "\n",
    "for id in tnrange(len(image_ids), desc = \"Processing images in dataset...\"):\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, EvalConfig(),\n",
    "                               image_ids[id], use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, EvalConfig()), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    \n",
    "    AR, positive_ids = utils.compute_recall(r[\"rois\"], gt_bbox, iou=0.2)\n",
    "    # Append AP to AP array\n",
    "    APs.append(AP)\n",
    "    ARs.append(AR)\n",
    "    \n",
    "    #F1_scores.append((2* (mean(precisions) * mean(recalls)))/(mean(precisions) + mean(recalls)))\n",
    "    np.mean(APs)\n",
    "    np.mean(ARs)\n",
    "    \n",
    "    # Append precisions\n",
    "    for precision in precisions:\n",
    "        precisions_arr.append(precision)\n",
    "    \n",
    "    # Append recalls\n",
    "    for recall in recalls:\n",
    "        recalls_arr.append(recall)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Append overlaps\n",
    "    for overlap in overlaps:\n",
    "        overlaps_arr.append(overlap)\n",
    "    \n",
    "    # Append class_ids\n",
    "    for class_id in r[\"class_ids\"]:\n",
    "        class_ids_arr.append(class_id)\n",
    "    \n",
    "    # Append scores \n",
    "    for score in r[\"scores\"]:\n",
    "        scores_arr.append(score)\n",
    "        \n",
    "        \n",
    "    \n",
    "mAP =  np.mean(APs)\n",
    "mAR = np.mean(ARs)\n",
    "print(\"mAP: \", mAP)\n",
    "print(\"mAR: \", mAR)\n",
    "\n",
    "\n",
    "F1_score_2 = (2 * mAP * mAR)/(mAP + mAR)\n",
    "print('second way calculate f1-score_2: ', F1_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO9BLgQhSRt0"
   },
   "outputs": [],
   "source": [
    "#Save the variables in a txt file \n",
    "file = open(MODEL_NAME+\"/\"+\"variable.txt\", \"w\")\n",
    "file.write(\"mAP = \" + str(mAP) + \"\\n\" +\"mAR = \"+ str(mAR) + \"\\n\"+\"F1 = \"+ str(F1_score_2) )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqQC-Ub6SRt0"
   },
   "source": [
    "## Plot precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qoH44TxSRt0",
    "outputId": "51722c4a-1888-4123-f61d-a513c4b94731"
   },
   "outputs": [],
   "source": [
    "visualize.plot_precision_recall(MODEL_NAME, AP, precisions, recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnIpcwQzSRt0"
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "2e0e30de00ce4e1ea446fb77711b64ff"
     ]
    },
    "id": "PndMv9f8SRt0",
    "outputId": "69303702-3f58-462a-8143-d2aebb65ab81"
   },
   "outputs": [],
   "source": [
    "# an example of plotting confusion matrix.\n",
    "# the first step consists of computing ground-truth and prediction vectors for all images.\n",
    "# using these vectors, the plot_confusion_matrix_from_data function plots the CM and computes tps fps and fns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "#ground-truth and predictions lists\n",
    "gt_tot = np.array([])\n",
    "pred_tot = np.array([])\n",
    "#mAP list\n",
    "mAP_ = []\n",
    "\n",
    "\n",
    "for id in tnrange(len(image_ids), desc = \"Processing images in dataset...\"):\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, EvalConfig(),\n",
    "                               image_ids[id], use_mini_mask=False)\n",
    "\n",
    "    # Run the model\n",
    "    results = model.detect([image], verbose=1)\n",
    "    r = results[0]\n",
    "    \n",
    "    #compute gt_tot and pred_tot\n",
    "    gt, pred = utils.gt_pred_lists(gt_class_id, gt_bbox, r['class_ids'], r['rois'])\n",
    "    gt_tot = np.append(gt_tot, gt)\n",
    "    pred_tot = np.append(pred_tot, pred)\n",
    "    \n",
    "    #precision_, recall_, AP_ \n",
    "    AP_, precision_, recall_, overlap_ = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                          r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "    #check if the vectors len are equal\n",
    "    print(\"the actual len of the gt vect is : \", len(gt_tot))\n",
    "    print(\"the actual len of the pred vect is : \", len(pred_tot))\n",
    "    \n",
    "    mAP_.append(AP_)\n",
    "    print(\"Average precision of this image : \",AP_)\n",
    "    print(\"The actual mean average precision for the whole images (matterport methode) \", sum(mAP_)/len(mAP_))\n",
    "   # print(\"Ground truth object : \"+dataset_val.class_names[gt])\n",
    "   # print(\"Predicted object : \"+dataset_val.class_names[pred])\n",
    "\n",
    "\n",
    "#print(\"ground truth list : \",gt_tot)\n",
    "#print(\"predicted list : \",pred_tot)\n",
    "\n",
    "tp,fp,fn, dm =utils.plot_confusion_matrix_from_data(MODEL_NAME, class_names, gt_tot,pred_tot,fz=18, figsize=(20,20), lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LhpHJLlSRt1"
   },
   "outputs": [],
   "source": [
    "image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22tCVF8sSRt1"
   },
   "outputs": [],
   "source": [
    "# https://vitalflux.com/ml-metrics-sensitivity-vs-specificity-difference/\n",
    "\n",
    "#Mathematically, sensitivity or true positive rate can be calculated as the following:\n",
    "\n",
    "#Sensitivity = (True Positive)/(True Positive + False Negative)\n",
    "\n",
    "Sensitivity =  sum(tp)/(sum(tp)+sum(fn))\n",
    "print(Sensitivity)\n",
    "\n",
    "\n",
    "# Mathematically, specificity can be calculated as the following:\n",
    "\n",
    "# Specificity = (True Negative)/(True Negative + False Positive)\n",
    "# \n",
    "#Save the variables in a txt file \n",
    "file = open(MODEL_NAME+\"/\"+\"Sensitivity.txt\", \"w\")\n",
    "file.write(\"Sensitivity = \" + str(Sensitivity)) \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgWKOcb3SRt1"
   },
   "outputs": [],
   "source": [
    "print(\"tp for each class :\",tp)\n",
    "print(\"fp for each class :\",fp)\n",
    "print(\"fn for each class :\",fn)\n",
    "\n",
    "#eliminate the background class (class A) from tps fns and fns lists since it doesn't concern us anymore : \n",
    "del tp[0]\n",
    "del fp[0]\n",
    "del fn[0]\n",
    "\n",
    "\n",
    "print(\"\\n########################\\n\")\n",
    "print(\"tp for each class :\",tp)\n",
    "print(\"fp for each class :\",fp)\n",
    "print(\"fn for each class :\",fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHjxeNkHSRt1"
   },
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%cd home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-v9mDAFSRt1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNEp6BIkSRt1"
   },
   "outputs": [],
   "source": [
    "path_report = MODEL_NAME + '/' + str(NUM_CLASSES) +  'classes_images'\n",
    "\n",
    "file_names = next(os.walk(TEST_IMAGE_DIR))[2]\n",
    "file = open(path_report+\"/\"+\"report_images.txt\", \"w\")\n",
    "\n",
    "\n",
    "for pic in file_names:\n",
    "#image = skimage.io.imread(os.path.join(TEST_IMAGE_DIR, random.choice(file_names)))\n",
    "    if pic.endswith('.jpg'):\n",
    "        path_new = MODEL_NAME + '/' + str(NUM_CLASSES) +  'classes_images/' + str(pic)\n",
    "        print(pic)\n",
    "        image = skimage.io.imread(os.path.join(TEST_IMAGE_DIR, pic))\n",
    "        #Save the variables in a txt file \n",
    "        file.write(str(pic)+\"\\n\")\n",
    "        file.write(\"--------------------------------------------\"+\"\\n\")\n",
    "\n",
    "\n",
    "        # Run the model\n",
    "        results = model.detect([image], verbose=1)\n",
    "        r = results[0]\n",
    "        flag = False\n",
    "\n",
    "        for i in range(0, len(r['class_ids'])):\n",
    "            if r['class_ids'][i] == 1:\n",
    "                Xm = (r['rois'][i][1] + r['rois'][i][3])/2\n",
    "                Ym = (r['rois'][i][0]+ r['rois'][i][2])/2\n",
    "                coord = [Xm,Ym]\n",
    "                print('A cancerigenous cell was found in X, Y: ', coord)\n",
    "                file.write(\"'A cancerigenous cell was found in X, Y: '\"+ str(coord)+\"\\n\")\n",
    "                flag = True\n",
    "\n",
    "\n",
    "\n",
    "            if r['class_ids'][i] == 2:\n",
    "                Xm = (r['rois'][i][1] + r['rois'][i][3])/2\n",
    "                Ym = (r['rois'][i][0]+ r['rois'][i][2])/2\n",
    "                coord = [Xm,Ym]\n",
    "                print('A dangerous cell was found in X, Y: ', coord)     \n",
    "                file.write(\"'A dangerous cell was found in X, Y: '\"+ str(coord)+\"\\n\")\n",
    "                flag = True\n",
    "\n",
    "\n",
    "        if flag == False:\n",
    "            file.write(\"'No dangerous/cancerigenous cell was found'\"+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        visualize.display_instances(path_new, image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "file.close()\n",
    "print(\"fim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EahvK1wSRt1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load a random image from the images folder\n",
    "file_names = next(os.walk(TEST_IMAGE_DIR))[2]\n",
    "rand = random.choice(file_names)\n",
    "image = skimage.io.imread(os.path.join(TEST_IMAGE_DIR, rand))\n",
    "path_new = MODEL_NAME + '/' + str(NUM_CLASSES) +  'classes_images/' + str(rand)\n",
    "\n",
    "#image = skimage.io.imread(\"cell_2_4400_0_4900_500_jpg.rf.634fc6434c65b180a5c3a9bc993e1241.jpg\")\n",
    "\n",
    "# Run detection\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "# Visualize results\n",
    "r = results[0]\n",
    "visualize.display_instances(path_new, image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
