{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# COCO related libraries\n",
    "from samples.coco import coco\n",
    "\n",
    "# MaskRCNN libraries\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "import mrcnn.V2_0_model_original as modellib\n",
    "#import mrcnn.model_resnext_2_0 as modellib\n",
    "\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#import time\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of classes in dataset. Must be of type integer\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# Relative path to .h5 weights file\n",
    "WEIGHTS_FILE = None\n",
    "#WEIGHTS_FILE = \"logs/v2_5_classes_resnetxt_161_4x4_epc_250_step_per_ep_220_val_steps_100_alldataset_aug_v1_alpha_1_520221201T0016/mask_rcnn_v2_5_classes_resnetxt_161_4x4_epc_250_step_per_ep_220_val_steps_100_alldataset_aug_v1_alpha_1_5_0250.h5\" \n",
    "# Relative path to annotations JSON file\n",
    "#TRAIN_ANNOTATIONS_FILE = \"datasets/aug_1/train/3_classes_train.json\"\n",
    "TRAIN_ANNOTATIONS_FILE = \"datasets/aug_1/train/train.json\"\n",
    "\n",
    "# Relative path to directory of images that pertain to annotations file\n",
    "TRAIN_ANNOTATION_IMAGE_DIR = 'datasets/aug_1/train'\n",
    "\n",
    "# Relative path to annotations JSON file\n",
    "#VALIDATION_ANNOTATIONS_FILE = \"datasets/aug_1/valid/3_classes_valid.json\"\n",
    "VALIDATION_ANNOTATIONS_FILE = \"datasets/aug_1/valid/valid.json\"\n",
    "\n",
    "\n",
    "# Relative path to directory of images that pertain to annotations file\n",
    "VALIDATION_ANNOTATION_IMAGE_DIR = 'datasets/aug_1/valid'\n",
    "\n",
    "# Number of epochs to train dataset on\n",
    "NUM_EPOCHS = 250\n",
    "\n",
    "#MODEL_NAME = \"5_classes_resnet_no_class_1\"\n",
    "\n",
    "MODEL_NAME = \"just try\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Select which GPU to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig(coco.CocoConfig):\n",
    "    \"\"\"Configuration for training where MRCNN has two mask layers\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = MODEL_NAME\n",
    "\n",
    "    # Train on 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + NUM_CLASSES\n",
    "    \n",
    "    #layers\n",
    "    FPN_CLASSIF_FC_LAYERS_SIZE = 1024\n",
    "    \n",
    "    # Min and max image dimensions\n",
    "    IMAGE_MIN_DIM = 1152\n",
    "    IMAGE_MAX_DIM = 1280\n",
    " #   IMAGE_MIN_DIM = 800\n",
    "#    IMAGE_MAX_DIM = 1024\n",
    "\n",
    "    # You can experiment with this number to see if it improves training\n",
    "    STEPS_PER_EPOCH = 220\n",
    "\n",
    "    # This is how often validation is run. If you are using too much hard drive space\n",
    "    # on saved models (in the MODEL_DIR), try making this value larger.\n",
    "    VALIDATION_STEPS = 100\n",
    "    \n",
    "    # Learning rate\n",
    "    LEARNING_RATE = 0.1\n",
    "\n",
    "    # Alpha for changing mask loss weights\n",
    "    ALPHA = 1.\n",
    "    \n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": ALPHA* 1.\n",
    "    }    \n",
    "    \n",
    "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
    "    # [\"resnet50\", \"resnet101\", \"resnet152\", \"resnet203\", \"resnetxt50\", \"resnetxt101\", \"resnetxt152\",  \"resnetxt203\"]\n",
    "    BACKBONE = 'resnet50'\n",
    "    \n",
    "    CARDINALITY = 32\n",
    "\n",
    "    # To be honest, I haven't taken the time to figure out what these do\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "\n",
    "    \n",
    "    # Changed to 512 because that's how many the original MaskRCNN paper used\n",
    "    TRAIN_ROIS_PER_IMAGE = 512# 200 #512\n",
    "    MAX_GT_INSTANCES = 256# 114 # 256\n",
    "    POST_NMS_ROIS_INFERENCE = 2000 #1000 # change 2000\n",
    "    POST_NMS_ROIS_TRAINING = 2000 \n",
    "    \n",
    "    DETECTION_MAX_INSTANCES = 400# 114 # 400\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "ALPHA                          1.0\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "CARDINALITY                    32\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        400\n",
      "DETECTION_MIN_CONFIDENCE       0.5\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1280\n",
      "IMAGE_META_SIZE                18\n",
      "IMAGE_MIN_DIM                  1152\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1280 1280    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.1\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               256\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           just try\n",
      "NUM_CLASSES                    6\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        2000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                220\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           512\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               100\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TrainConfig().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class to load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoLikeDataset(utils.Dataset):\n",
    "    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n",
    "        See http://cocodataset.org/#home for more information.\n",
    "    \"\"\"\n",
    "    def load_data(self, annotation_json, images_dir):\n",
    "        \"\"\" Load the coco-like dataset from json\n",
    "        Args:\n",
    "            annotation_json: The path to the coco annotations json file\n",
    "            images_dir: The directory holding the images referred to by the json file\n",
    "        \"\"\"\n",
    "        # Load json from file\n",
    "        json_file = open(annotation_json)\n",
    "        coco_json = json.load(json_file)\n",
    "        json_file.close()\n",
    "        \n",
    "        # Add the class names using the base method from utils.Dataset\n",
    "        source_name = \"coco_like\"\n",
    "        for category in coco_json['categories']:\n",
    "            class_id = category['id']\n",
    "            class_name = category['name']\n",
    "            if class_id < 1:\n",
    "                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n",
    "                return\n",
    "            \n",
    "            self.add_class(source_name, class_id, class_name)\n",
    "        \n",
    "        # Get all annotations\n",
    "        annotations = {}\n",
    "        for annotation in coco_json['annotations']:\n",
    "            image_id = annotation['image_id']\n",
    "            if image_id not in annotations:\n",
    "                annotations[image_id] = []\n",
    "            annotations[image_id].append(annotation)\n",
    "        \n",
    "        # Get all images and add them to the dataset\n",
    "        seen_images = {}\n",
    "        for image in coco_json['images']:\n",
    "            image_id = image['id']\n",
    "            if image_id in seen_images:\n",
    "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
    "            else:\n",
    "                seen_images[image_id] = image\n",
    "                try:\n",
    "                    image_file_name = image['file_name']\n",
    "                    image_width = image['width']\n",
    "                    image_height = image['height']\n",
    "                except KeyError as key:\n",
    "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
    "                \n",
    "                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n",
    "                image_annotations = annotations[image_id]\n",
    "                \n",
    "                # Add the image using the base method from utils.Dataset\n",
    "                self.add_image(\n",
    "                    source=source_name,\n",
    "                    image_id=image_id,\n",
    "                    path=image_path,\n",
    "                    width=image_width,\n",
    "                    height=image_height,\n",
    "                    annotations=image_annotations\n",
    "                )\n",
    "                \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\" Load instance masks for the given image.\n",
    "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
    "        Args:\n",
    "            image_id: The id of the image to load masks for\n",
    "        Returns:\n",
    "            masks: A bool array of shape [height, width, instance count] with\n",
    "                one mask per instance.\n",
    "            class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        image_info = self.image_info[image_id]\n",
    "        annotations = image_info['annotations']\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            class_id = annotation['category_id']\n",
    "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
    "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
    "            for segmentation in annotation['segmentation']:\n",
    "                mask_draw.polygon(segmentation, fill=1)\n",
    "                bool_array = np.array(mask) > 0\n",
    "                instance_masks.append(bool_array)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        mask = np.dstack(instance_masks)\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "        \n",
    "        return mask, class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "107",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset_train\u001b[38;5;241m.\u001b[39mprepare()\n\u001b[1;32m      5\u001b[0m dataset_val \u001b[38;5;241m=\u001b[39m CocoLikeDataset()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdataset_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVALIDATION_ANNOTATIONS_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVALIDATION_ANNOTATION_IMAGE_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m dataset_val\u001b[38;5;241m.\u001b[39mprepare()\n",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m, in \u001b[0;36mCocoLikeDataset.load_data\u001b[0;34m(self, annotation_json, images_dir)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Skipping image (id: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) with missing key: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(image_id, key))\n\u001b[1;32m     50\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(images_dir, image_file_name))\n\u001b[0;32m---> 51\u001b[0m image_annotations \u001b[38;5;241m=\u001b[39m \u001b[43mannotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Add the image using the base method from utils.Dataset\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_image(\n\u001b[1;32m     55\u001b[0m     source\u001b[38;5;241m=\u001b[39msource_name,\n\u001b[1;32m     56\u001b[0m     image_id\u001b[38;5;241m=\u001b[39mimage_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     annotations\u001b[38;5;241m=\u001b[39mimage_annotations\n\u001b[1;32m     61\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 107"
     ]
    }
   ],
   "source": [
    "dataset_train = CocoLikeDataset()\n",
    "dataset_train.load_data(TRAIN_ANNOTATIONS_FILE, TRAIN_ANNOTATION_IMAGE_DIR)\n",
    "dataset_train.prepare()\n",
    "\n",
    "dataset_val = CocoLikeDataset()\n",
    "dataset_val.load_data(VALIDATION_ANNOTATIONS_FILE, VALIDATION_ANNOTATION_IMAGE_DIR)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MaskRCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode = \"training\", config = TrainConfig(), model_dir = MODEL_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights into model if weights file is not None\n",
    "### This is meant to be used if you are refining on a set of preexisting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WEIGHTS_FILE is not None:\n",
    "    model.load_weights(WEIGHTS_FILE, by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "### The model after each epoch will be saved in the logs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_train = time.time()\n",
    "model.train(dataset_train, dataset_val, learning_rate = TrainConfig().LEARNING_RATE, epochs = NUM_EPOCHS, layers = 'all')\n",
    "history = model.keras_model.history.history\n",
    "\n",
    "end_train = time.time()\n",
    "minutes = round((end_train - start_train) / 60, 2)\n",
    "print(f'Training took {minutes} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.makedirs(MODEL_NAME)\n",
    "%cd $MODEL_NAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "epochs = range(1,len(next(iter(history.values())))+1)\n",
    "df = pd.DataFrame(history, index=epochs)\n",
    "df.to_csv(MODEL_NAME+'.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(epochs, history[\"loss\"], label=\"Train loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\n",
    "plt.ylim([0,5])\n",
    "plt.legend()\n",
    "plt.subplot(222)\n",
    "plt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train class loss\")\n",
    "plt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid class loss\")\n",
    "plt.ylim([0,5])\n",
    "plt.legend()\n",
    "plt.subplot(223)\n",
    "plt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train box loss\")\n",
    "plt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid box loss\")\n",
    "plt.ylim([0,5])\n",
    "plt.legend()\n",
    "plt.subplot(224)\n",
    "plt.plot(epochs, history[\"mrcnn_mask_loss\"], label=\"Train mask loss\")\n",
    "plt.plot(epochs, history[\"val_mrcnn_mask_loss\"], label=\"Valid mask loss\")\n",
    "plt.ylim([0,5])\n",
    "plt.legend()\n",
    "plt.savefig(\"Losses3\")\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include evaluation scripts in training script so that the kernel does not have to be reloaded. Eases the process of rapidly training and evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO related libraries\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "from samples.coco import coco\n",
    "from samples.coco.coco import evaluate_coco\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import skimage.io\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%cd home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in dataset. Must be of type integer\n",
    "#NUM_CLASSES = 3\n",
    "\n",
    "# Relative path to .h5 weights file\n",
    "#WEIGHTS_FILE = None\n",
    "\n",
    "#WEIGHTS_FILE = 'logs/5_classes_resnetxt_152_epc_200_step_per_ep_220_val_steps_200_alldataset_aug20221012T0013/mask_rcnn_5_classes_resnetxt_152_epc_200_step_per_ep_220_val_steps_200_alldataset_aug_0200.h5'\n",
    "\n",
    "# Relative path to ground truth annotations JSON filecccccccccccccccccccccccccccccccccc\n",
    "TEST_ANNOTATIONS_FILE = 'datasets/no_aug/test/test.json'\n",
    "\n",
    "# Relative path to images associated with ground truth JSON fileccccccccccccc\n",
    "TEST_DATASET_DIR = 'datasets/no_aug/test/'\n",
    "\n",
    "# Relative path to the directory of images that you want to run inferencing on\n",
    "TEST_IMAGE_DIR = 'datasets/no_aug/test/'\n",
    "\n",
    "#MODEL_NAME = \"5_classes_resnetxt_152_epc_200_step_per_ep_220_val_steps_200_alldataset_aug20221012T0013\"\n",
    "\n",
    "#MODEL_NAME = \"152_testv2_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare evaluation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalConfig(coco.CocoConfig):\n",
    "    \"\"\" Configuration for evaluation \"\"\"\n",
    "    \n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = MODEL_NAME\n",
    "    \n",
    "    # How many GPUs\n",
    "    GPU_COUNT = 1\n",
    "    \n",
    "    # How many images per gpu\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + NUM_CLASSES  # background + other classes\n",
    "    \n",
    "    #layers\n",
    "    FPN_CLASSIF_FC_LAYERS_SIZE = 1024\n",
    "    \n",
    "    IMAGE_MIN_DIM = 1152\n",
    "    IMAGE_MAX_DIM = 1280\n",
    "    \n",
    "    # Alpha for changing mask loss weights\n",
    "    ALPHA =1.\n",
    "    \n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": ALPHA* 1.\n",
    "    }    \n",
    "    \n",
    "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
    "    # [\"resnet50\", \"resnet101\", \"resnet152\", \"resnet203\", \"resnetxt50\", \"resnetxt101\", \"resnetxt152\",  \"resnetxt203\"]\n",
    "    BACKBONE = 'resnet50'\n",
    "    \n",
    "    CARDINALITY = 32\n",
    "\n",
    "\n",
    "\n",
    "    # To be honest, I haven't taken the time to figure out what these do\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    \n",
    "    # Changed to 512 because that's how many the original MaskRCNN paper used\n",
    "    TRAIN_ROIS_PER_IMAGE = 512#200\n",
    "    MAX_GT_INSTANCES = 256#114\n",
    "    POST_NMS_ROIS_INFERENCE = 2000#1000 \n",
    "    POST_NMS_ROIS_TRAINING = 2000 \n",
    "    \n",
    "    DETECTION_MAX_INSTANCES = 400#114\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalConfig().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build class to load ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(utils.Dataset):\n",
    "    def load_coco_gt(self, annotations_file, dataset_dir):\n",
    "        \"\"\"Load a COCO styled ground truth dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create COCO object\n",
    "        coco = COCO(annotations_file)\n",
    "\n",
    "        # Load all classes\n",
    "        class_ids = sorted(coco.getCatIds())\n",
    "\n",
    "        # Load all images\n",
    "        image_ids = list(coco.imgs.keys())\n",
    "\n",
    "        # Add classes\n",
    "        for i in class_ids:\n",
    "            self.add_class(\"coco\", i, coco.loadCats(i)[0][\"name\"])\n",
    "\n",
    "        # Add images\n",
    "        for i in image_ids:\n",
    "            self.add_image(\n",
    "                \"coco\", image_id = i,\n",
    "                path = os.path.join(dataset_dir, coco.imgs[i]['file_name']),\n",
    "                width = coco.imgs[i][\"width\"],\n",
    "                height = coco.imgs[i][\"height\"],\n",
    "                annotations = coco.loadAnns(coco.getAnnIds(\n",
    "                    imgIds = [i], catIds = class_ids, iscrowd=None)))\n",
    "        \n",
    "        return coco\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"coco\":\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"coco.{}\".format(annotation['category_id']))\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"coco\":\n",
    "            return \"http://cocodataset.org/#explore?id={}\".format(info[\"id\"])\n",
    "        else:\n",
    "            super(CocoDataset, self).image_reference(image_id)\n",
    "\n",
    "    # The following two functions are from pycocotools with a few changes.\n",
    "\n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MaskRCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the model in inference mode\n",
    "model = modellib.MaskRCNN(mode = \"inference\", config = EvalConfig(), model_dir = MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights from last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WEIGHTS_FILE is None:\n",
    "    model.load_weights(model.find_last(), by_name = True)\n",
    "else:\n",
    "    model.load_weights(WEIGHTS_FILE, by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_val = CocoDataset()\n",
    "coco = dataset_val.load_coco_gt(annotations_file = TEST_ANNOTATIONS_FILE, dataset_dir = TEST_DATASET_DIR)\n",
    "dataset_val.prepare()\n",
    "class_names = dataset_val.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model with COCO test\n",
    "### If your results come back as a bunch of zeros, check to make sure that the \"width\" and \"height\" tag in your COCO dataset are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_coco(model, dataset_val, coco, \"segm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating mAP as per example in train_shapes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, len(dataset_val.image_ids))\n",
    "\n",
    "# Instanciate arrays to create our metrics\n",
    "APs = []\n",
    "ARs = []\n",
    "precisions_arr = []\n",
    "recalls_arr = []\n",
    "overlaps_arr = []\n",
    "class_ids_arr = []\n",
    "scores_arr = []\n",
    "F1_scores = []; \n",
    "\n",
    "for id in tnrange(len(image_ids), desc = \"Processing images in dataset...\"):\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, EvalConfig(),\n",
    "                               image_ids[id], use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, EvalConfig()), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    \n",
    "    AR, positive_ids = utils.compute_recall(r[\"rois\"], gt_bbox, iou=0.2)\n",
    "    # Append AP to AP array\n",
    "    APs.append(AP)\n",
    "    ARs.append(AR)\n",
    "    \n",
    "    #F1_scores.append((2* (mean(precisions) * mean(recalls)))/(mean(precisions) + mean(recalls)))\n",
    "    np.mean(APs)\n",
    "    np.mean(ARs)\n",
    "    \n",
    "    # Append precisions\n",
    "    for precision in precisions:\n",
    "        precisions_arr.append(precision)\n",
    "    \n",
    "    # Append recalls\n",
    "    for recall in recalls:\n",
    "        recalls_arr.append(recall)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Append overlaps\n",
    "    for overlap in overlaps:\n",
    "        overlaps_arr.append(overlap)\n",
    "    \n",
    "    # Append class_ids\n",
    "    for class_id in r[\"class_ids\"]:\n",
    "        class_ids_arr.append(class_id)\n",
    "    \n",
    "    # Append scores \n",
    "    for score in r[\"scores\"]:\n",
    "        scores_arr.append(score)\n",
    "        \n",
    "        \n",
    "    \n",
    "mAP =  np.mean(APs)\n",
    "mAR = np.mean(ARs)\n",
    "print(\"mAP: \", mAP)\n",
    "print(\"mAR: \", mAR)\n",
    "\n",
    "\n",
    "F1_score_2 = (2 * mAP * mAR)/(mAP + mAR)\n",
    "print('second way calculate f1-score_2: ', F1_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the variables in a txt file \n",
    "file = open(MODEL_NAME+\"/\"+\"variable.txt\", \"w\")\n",
    "file.write(\"mAP = \" + str(mAP) + \"\\n\" +\"mAR = \"+ str(mAR) + \"\\n\"+\"F1 = \"+ str(F1_score_2) )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.plot_precision_recall(MODEL_NAME, AP, precisions, recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of plotting confusion matrix.\n",
    "# the first step consists of computing ground-truth and prediction vectors for all images.\n",
    "# using these vectors, the plot_confusion_matrix_from_data function plots the CM and computes tps fps and fns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "#ground-truth and predictions lists\n",
    "gt_tot = np.array([])\n",
    "pred_tot = np.array([])\n",
    "#mAP list\n",
    "mAP_ = []\n",
    "\n",
    "\n",
    "for id in tnrange(len(image_ids), desc = \"Processing images in dataset...\"):\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, EvalConfig(),\n",
    "                               image_ids[id], use_mini_mask=False)\n",
    "\n",
    "    # Run the model\n",
    "    results = model.detect([image], verbose=1)\n",
    "    r = results[0]\n",
    "    \n",
    "    #compute gt_tot and pred_tot\n",
    "    gt, pred = utils.gt_pred_lists(gt_class_id, gt_bbox, r['class_ids'], r['rois'])\n",
    "    gt_tot = np.append(gt_tot, gt)\n",
    "    pred_tot = np.append(pred_tot, pred)\n",
    "    \n",
    "    #precision_, recall_, AP_ \n",
    "    AP_, precision_, recall_, overlap_ = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                          r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "    #check if the vectors len are equal\n",
    "    print(\"the actual len of the gt vect is : \", len(gt_tot))\n",
    "    print(\"the actual len of the pred vect is : \", len(pred_tot))\n",
    "    \n",
    "    mAP_.append(AP_)\n",
    "    print(\"Average precision of this image : \",AP_)\n",
    "    print(\"The actual mean average precision for the whole images (matterport methode) \", sum(mAP_)/len(mAP_))\n",
    "   # print(\"Ground truth object : \"+dataset_val.class_names[gt])\n",
    "   # print(\"Predicted object : \"+dataset_val.class_names[pred])\n",
    "\n",
    "\n",
    "#print(\"ground truth list : \",gt_tot)\n",
    "#print(\"predicted list : \",pred_tot)\n",
    "\n",
    "tp,fp,fn, dm =utils.plot_confusion_matrix_from_data(MODEL_NAME, class_names, gt_tot,pred_tot,fz=18, figsize=(20,20), lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://vitalflux.com/ml-metrics-sensitivity-vs-specificity-difference/\n",
    "\n",
    "#Mathematically, sensitivity or true positive rate can be calculated as the following:\n",
    "\n",
    "#Sensitivity = (True Positive)/(True Positive + False Negative)\n",
    "\n",
    "Sensitivity =  tp/(tp+fn)\n",
    "\n",
    "\n",
    "# Mathematically, specificity can be calculated as the following:\n",
    "\n",
    "# Specificity = (True Negative)/(True Negative + False Positive)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tp for each class :\",tp)\n",
    "print(\"fp for each class :\",fp)\n",
    "print(\"fn for each class :\",fn)\n",
    "\n",
    "#eliminate the background class (class A) from tps fns and fns lists since it doesn't concern us anymore : \n",
    "del tp[0]\n",
    "del fp[0]\n",
    "del fn[0]\n",
    "\n",
    "\n",
    "print(\"\\n########################\\n\")\n",
    "print(\"tp for each class :\",tp)\n",
    "print(\"fp for each class :\",fp)\n",
    "print(\"fn for each class :\",fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%cd home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_names = next(os.walk(TEST_IMAGE_DIR))[2]\n",
    "image = skimage.io.imread(os.path.join(TEST_IMAGE_DIR, random.choice(file_names)))\n",
    "\n",
    "#image = skimage.io.imread(os.path.join(TEST_IMAGE_DIR,\"cell_2_5600_1184_6100_1684_jpg.rf.008a0e7fe4d7df8bba2675d172a2da69.jpg\"))\n",
    "\n",
    "# Run the model\n",
    "results = model.detect([image], verbose=1)\n",
    "r = results[0]\n",
    "\n",
    "for i in range(0, len(r['class_ids'])):\n",
    "    if r['class_ids'][i] == 1:\n",
    "        Xm = (r['rois'][i][1] + r['rois'][i][3])/2\n",
    "        Ym = (r['rois'][i][0]+ r['rois'][i][2])/2\n",
    "        coord = [Xm,Ym]\n",
    "        print('A cancerigenous cell was found in X, Y: ', coord)\n",
    "        \n",
    "    if r['class_ids'][i] == 2:\n",
    "        Xm = (r['rois'][i][1] + r['rois'][i][3])/2\n",
    "        Ym = (r['rois'][i][0]+ r['rois'][i][2])/2\n",
    "        coord = [Xm,Ym]\n",
    "        print('A dangerous cell was found in X, Y: ', coord)        \n",
    "\n",
    "\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a random image from the images folder\n",
    "file_names = next(os.walk(TEST_IMAGE_DIR))[2]\n",
    "image = skimage.io.imread(os.path.join(TEST_IMAGE_DIR, random.choice(file_names)))\n",
    "\n",
    "#image = skimage.io.imread(\"cell_2_4400_0_4900_500_jpg.rf.634fc6434c65b180a5c3a9bc993e1241.jpg\")\n",
    "\n",
    "# Run detection\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "# Visualize results\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
